{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Above ear - pull hair', 'Cheek - pinch skin',\n",
       "       'Drink from bottle/cup', 'Eyebrow - pull hair',\n",
       "       'Eyelash - pull hair',\n",
       "       'Feel around in tray and pull out an object',\n",
       "       'Forehead - pull hairline', 'Forehead - scratch', 'Glasses on/off',\n",
       "       'Neck - pinch skin', 'Neck - scratch', 'Pinch knee/leg skin',\n",
       "       'Pull air toward your face', 'Scratch knee/leg skin',\n",
       "       'Text on phone', 'Wave hello', 'Write name in air',\n",
       "       'Write name on leg'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import xgboost as xgb\n",
    "cmi_imu_18_model = load_model(\"cmi_imu_18_3.h5\")\n",
    "cmi_thermal_model = load_model(\"cmi_thermal_18.h5\")\n",
    "cmi_tof_model = load_model(\"cmi_tof_18.h5\")\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(\"xgb_model_679_8385.json\")\n",
    "\n",
    "classes = np.load(\"imu_18_classes.npy\", allow_pickle=True)\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: 'Above ear - pull hair'\n",
      "✅ Found: 'Forehead - pull hairline'\n",
      "✅ Found: 'Forehead - scratch'\n",
      "✅ Found: 'Eyebrow - pull hair'\n",
      "✅ Found: 'Eyelash - pull hair'\n",
      "✅ Found: 'Neck - pinch skin'\n",
      "✅ Found: 'Neck - scratch'\n",
      "✅ Found: 'Cheek - pinch skin'\n"
     ]
    }
   ],
   "source": [
    "target_classes = [\n",
    "    'Above ear - pull hair',\n",
    "    'Forehead - pull hairline',\n",
    "    'Forehead - scratch',\n",
    "    'Eyebrow - pull hair',\n",
    "    'Eyelash - pull hair',\n",
    "    'Neck - pinch skin',\n",
    "    'Neck - scratch',\n",
    "    'Cheek - pinch skin'\n",
    "]\n",
    "for t_class in target_classes:\n",
    "    if t_class not in classes:\n",
    "        print(f\"⚠️ Target class NOT found in class_names: '{t_class}'\")\n",
    "    else:\n",
    "        print(f\"✅ Found: '{t_class}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 5 320 2\n"
     ]
    }
   ],
   "source": [
    "df_imu_columns= [\n",
    " 'acc_x',\n",
    " 'acc_y',\n",
    " 'acc_z',\n",
    " 'rot_w',\n",
    " 'rot_x',\n",
    " 'rot_y',\n",
    " 'rot_z']\n",
    "\n",
    "df_thermal_columns= [\n",
    " 'thm_1',\n",
    " 'thm_2',\n",
    " 'thm_3',\n",
    " 'thm_4',\n",
    " 'thm_5'\n",
    "]\n",
    "\n",
    "df_tof_columns = [   \n",
    " 'tof_1_v0',\n",
    " 'tof_1_v1',\n",
    " 'tof_1_v2',\n",
    " 'tof_1_v3',\n",
    " 'tof_1_v4',\n",
    " 'tof_1_v5',\n",
    " 'tof_1_v6',\n",
    " 'tof_1_v7',\n",
    " 'tof_1_v8',\n",
    " 'tof_1_v9',\n",
    " 'tof_1_v10',\n",
    " 'tof_1_v11',\n",
    " 'tof_1_v12',\n",
    " 'tof_1_v13',\n",
    " 'tof_1_v14',\n",
    " 'tof_1_v15',\n",
    " 'tof_1_v16',\n",
    " 'tof_1_v17',\n",
    " 'tof_1_v18',\n",
    " 'tof_1_v19',\n",
    " 'tof_1_v20',\n",
    " 'tof_1_v21',\n",
    " 'tof_1_v22',\n",
    " 'tof_1_v23',\n",
    " 'tof_1_v24',\n",
    " 'tof_1_v25',\n",
    " 'tof_1_v26',\n",
    " 'tof_1_v27',\n",
    " 'tof_1_v28',\n",
    " 'tof_1_v29',\n",
    " 'tof_1_v30',\n",
    " 'tof_1_v31',\n",
    " 'tof_1_v32',\n",
    " 'tof_1_v33',\n",
    " 'tof_1_v34',\n",
    " 'tof_1_v35',\n",
    " 'tof_1_v36',\n",
    " 'tof_1_v37',\n",
    " 'tof_1_v38',\n",
    " 'tof_1_v39',\n",
    " 'tof_1_v40',\n",
    " 'tof_1_v41',\n",
    " 'tof_1_v42',\n",
    " 'tof_1_v43',\n",
    " 'tof_1_v44',\n",
    " 'tof_1_v45',\n",
    " 'tof_1_v46',\n",
    " 'tof_1_v47',\n",
    " 'tof_1_v48',\n",
    " 'tof_1_v49',\n",
    " 'tof_1_v50',\n",
    " 'tof_1_v51',\n",
    " 'tof_1_v52',\n",
    " 'tof_1_v53',\n",
    " 'tof_1_v54',\n",
    " 'tof_1_v55',\n",
    " 'tof_1_v56',\n",
    " 'tof_1_v57',\n",
    " 'tof_1_v58',\n",
    " 'tof_1_v59',\n",
    " 'tof_1_v60',\n",
    " 'tof_1_v61',\n",
    " 'tof_1_v62',\n",
    " 'tof_1_v63',\n",
    " 'tof_2_v0',\n",
    " 'tof_2_v1',\n",
    " 'tof_2_v2',\n",
    " 'tof_2_v3',\n",
    " 'tof_2_v4',\n",
    " 'tof_2_v5',\n",
    " 'tof_2_v6',\n",
    " 'tof_2_v7',\n",
    " 'tof_2_v8',\n",
    " 'tof_2_v9',\n",
    " 'tof_2_v10',\n",
    " 'tof_2_v11',\n",
    " 'tof_2_v12',\n",
    " 'tof_2_v13',\n",
    " 'tof_2_v14',\n",
    " 'tof_2_v15',\n",
    " 'tof_2_v16',\n",
    " 'tof_2_v17',\n",
    " 'tof_2_v18',\n",
    " 'tof_2_v19',\n",
    " 'tof_2_v20',\n",
    " 'tof_2_v21',\n",
    " 'tof_2_v22',\n",
    " 'tof_2_v23',\n",
    " 'tof_2_v24',\n",
    " 'tof_2_v25',\n",
    " 'tof_2_v26',\n",
    " 'tof_2_v27',\n",
    " 'tof_2_v28',\n",
    " 'tof_2_v29',\n",
    " 'tof_2_v30',\n",
    " 'tof_2_v31',\n",
    " 'tof_2_v32',\n",
    " 'tof_2_v33',\n",
    " 'tof_2_v34',\n",
    " 'tof_2_v35',\n",
    " 'tof_2_v36',\n",
    " 'tof_2_v37',\n",
    " 'tof_2_v38',\n",
    " 'tof_2_v39',\n",
    " 'tof_2_v40',\n",
    " 'tof_2_v41',\n",
    " 'tof_2_v42',\n",
    " 'tof_2_v43',\n",
    " 'tof_2_v44',\n",
    " 'tof_2_v45',\n",
    " 'tof_2_v46',\n",
    " 'tof_2_v47',\n",
    " 'tof_2_v48',\n",
    " 'tof_2_v49',\n",
    " 'tof_2_v50',\n",
    " 'tof_2_v51',\n",
    " 'tof_2_v52',\n",
    " 'tof_2_v53',\n",
    " 'tof_2_v54',\n",
    " 'tof_2_v55',\n",
    " 'tof_2_v56',\n",
    " 'tof_2_v57',\n",
    " 'tof_2_v58',\n",
    " 'tof_2_v59',\n",
    " 'tof_2_v60',\n",
    " 'tof_2_v61',\n",
    " 'tof_2_v62',\n",
    " 'tof_2_v63',\n",
    " 'tof_3_v0',\n",
    " 'tof_3_v1',\n",
    " 'tof_3_v2',\n",
    " 'tof_3_v3',\n",
    " 'tof_3_v4',\n",
    " 'tof_3_v5',\n",
    " 'tof_3_v6',\n",
    " 'tof_3_v7',\n",
    " 'tof_3_v8',\n",
    " 'tof_3_v9',\n",
    " 'tof_3_v10',\n",
    " 'tof_3_v11',\n",
    " 'tof_3_v12',\n",
    " 'tof_3_v13',\n",
    " 'tof_3_v14',\n",
    " 'tof_3_v15',\n",
    " 'tof_3_v16',\n",
    " 'tof_3_v17',\n",
    " 'tof_3_v18',\n",
    " 'tof_3_v19',\n",
    " 'tof_3_v20',\n",
    " 'tof_3_v21',\n",
    " 'tof_3_v22',\n",
    " 'tof_3_v23',\n",
    " 'tof_3_v24',\n",
    " 'tof_3_v25',\n",
    " 'tof_3_v26',\n",
    " 'tof_3_v27',\n",
    " 'tof_3_v28',\n",
    " 'tof_3_v29',\n",
    " 'tof_3_v30',\n",
    " 'tof_3_v31',\n",
    " 'tof_3_v32',\n",
    " 'tof_3_v33',\n",
    " 'tof_3_v34',\n",
    " 'tof_3_v35',\n",
    " 'tof_3_v36',\n",
    " 'tof_3_v37',\n",
    " 'tof_3_v38',\n",
    " 'tof_3_v39',\n",
    " 'tof_3_v40',\n",
    " 'tof_3_v41',\n",
    " 'tof_3_v42',\n",
    " 'tof_3_v43',\n",
    " 'tof_3_v44',\n",
    " 'tof_3_v45',\n",
    " 'tof_3_v46',\n",
    " 'tof_3_v47',\n",
    " 'tof_3_v48',\n",
    " 'tof_3_v49',\n",
    " 'tof_3_v50',\n",
    " 'tof_3_v51',\n",
    " 'tof_3_v52',\n",
    " 'tof_3_v53',\n",
    " 'tof_3_v54',\n",
    " 'tof_3_v55',\n",
    " 'tof_3_v56',\n",
    " 'tof_3_v57',\n",
    " 'tof_3_v58',\n",
    " 'tof_3_v59',\n",
    " 'tof_3_v60',\n",
    " 'tof_3_v61',\n",
    " 'tof_3_v62',\n",
    " 'tof_3_v63',\n",
    " 'tof_4_v0',\n",
    " 'tof_4_v1',\n",
    " 'tof_4_v2',\n",
    " 'tof_4_v3',\n",
    " 'tof_4_v4',\n",
    " 'tof_4_v5',\n",
    " 'tof_4_v6',\n",
    " 'tof_4_v7',\n",
    " 'tof_4_v8',\n",
    " 'tof_4_v9',\n",
    " 'tof_4_v10',\n",
    " 'tof_4_v11',\n",
    " 'tof_4_v12',\n",
    " 'tof_4_v13',\n",
    " 'tof_4_v14',\n",
    " 'tof_4_v15',\n",
    " 'tof_4_v16',\n",
    " 'tof_4_v17',\n",
    " 'tof_4_v18',\n",
    " 'tof_4_v19',\n",
    " 'tof_4_v20',\n",
    " 'tof_4_v21',\n",
    " 'tof_4_v22',\n",
    " 'tof_4_v23',\n",
    " 'tof_4_v24',\n",
    " 'tof_4_v25',\n",
    " 'tof_4_v26',\n",
    " 'tof_4_v27',\n",
    " 'tof_4_v28',\n",
    " 'tof_4_v29',\n",
    " 'tof_4_v30',\n",
    " 'tof_4_v31',\n",
    " 'tof_4_v32',\n",
    " 'tof_4_v33',\n",
    " 'tof_4_v34',\n",
    " 'tof_4_v35',\n",
    " 'tof_4_v36',\n",
    " 'tof_4_v37',\n",
    " 'tof_4_v38',\n",
    " 'tof_4_v39',\n",
    " 'tof_4_v40',\n",
    " 'tof_4_v41',\n",
    " 'tof_4_v42',\n",
    " 'tof_4_v43',\n",
    " 'tof_4_v44',\n",
    " 'tof_4_v45',\n",
    " 'tof_4_v46',\n",
    " 'tof_4_v47',\n",
    " 'tof_4_v48',\n",
    " 'tof_4_v49',\n",
    " 'tof_4_v50',\n",
    " 'tof_4_v51',\n",
    " 'tof_4_v52',\n",
    " 'tof_4_v53',\n",
    " 'tof_4_v54',\n",
    " 'tof_4_v55',\n",
    " 'tof_4_v56',\n",
    " 'tof_4_v57',\n",
    " 'tof_4_v58',\n",
    " 'tof_4_v59',\n",
    " 'tof_4_v60',\n",
    " 'tof_4_v61',\n",
    " 'tof_4_v62',\n",
    " 'tof_4_v63',\n",
    " 'tof_5_v0',\n",
    " 'tof_5_v1',\n",
    " 'tof_5_v2',\n",
    " 'tof_5_v3',\n",
    " 'tof_5_v4',\n",
    " 'tof_5_v5',\n",
    " 'tof_5_v6',\n",
    " 'tof_5_v7',\n",
    " 'tof_5_v8',\n",
    " 'tof_5_v9',\n",
    " 'tof_5_v10',\n",
    " 'tof_5_v11',\n",
    " 'tof_5_v12',\n",
    " 'tof_5_v13',\n",
    " 'tof_5_v14',\n",
    " 'tof_5_v15',\n",
    " 'tof_5_v16',\n",
    " 'tof_5_v17',\n",
    " 'tof_5_v18',\n",
    " 'tof_5_v19',\n",
    " 'tof_5_v20',\n",
    " 'tof_5_v21',\n",
    " 'tof_5_v22',\n",
    " 'tof_5_v23',\n",
    " 'tof_5_v24',\n",
    " 'tof_5_v25',\n",
    " 'tof_5_v26',\n",
    " 'tof_5_v27',\n",
    " 'tof_5_v28',\n",
    " 'tof_5_v29',\n",
    " 'tof_5_v30',\n",
    " 'tof_5_v31',\n",
    " 'tof_5_v32',\n",
    " 'tof_5_v33',\n",
    " 'tof_5_v34',\n",
    " 'tof_5_v35',\n",
    " 'tof_5_v36',\n",
    " 'tof_5_v37',\n",
    " 'tof_5_v38',\n",
    " 'tof_5_v39',\n",
    " 'tof_5_v40',\n",
    " 'tof_5_v41',\n",
    " 'tof_5_v42',\n",
    " 'tof_5_v43',\n",
    " 'tof_5_v44',\n",
    " 'tof_5_v45',\n",
    " 'tof_5_v46',\n",
    " 'tof_5_v47',\n",
    " 'tof_5_v48',\n",
    " 'tof_5_v49',\n",
    " 'tof_5_v50',\n",
    " 'tof_5_v51',\n",
    " 'tof_5_v52',\n",
    " 'tof_5_v53',\n",
    " 'tof_5_v54',\n",
    " 'tof_5_v55',\n",
    " 'tof_5_v56',\n",
    " 'tof_5_v57',\n",
    " 'tof_5_v58',\n",
    " 'tof_5_v59',\n",
    " 'tof_5_v60',\n",
    " 'tof_5_v61',\n",
    " 'tof_5_v62',\n",
    " 'tof_5_v63']\n",
    "\n",
    "df_labels_columns = ['sequence_id','gesture_encoded']\n",
    "\n",
    "print(len(df_imu_columns), len(df_thermal_columns),len(df_tof_columns),len(df_labels_columns) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_whole_sequences_sumbission(df_train,df_imu_columns,df_thermal_columns,df_tof_columns,max_steps=400):\n",
    "\n",
    "    nan_columns_X = df_train.columns[df_train.isnull().any()].tolist()\n",
    "    print(\"Columns in X with NaNs Before:\\n\", nan_columns_X)\n",
    "    df_train = df_train.infer_objects(copy=False)\n",
    "    df_train[df_train.select_dtypes(include=[\"number\"]).columns] = df_train.select_dtypes(include=[\"number\"]).interpolate(method=\"linear\", axis=0)\n",
    "    nan_columns_X = df_train.columns[df_train.isnull().any()].tolist()\n",
    "    print(\"Columns in X with NaNs After:\\n\", nan_columns_X)\n",
    "    \n",
    "    df_train = df_train.sort_values(by=[\"sequence_id\", \"sequence_counter\"], \n",
    "                    ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    # Drop unnecessary columns early\n",
    "    df_train = df_train.drop(columns=[\"sequence_counter\"])  \n",
    "    \n",
    "    n_imu_features = len(df_imu_columns)\n",
    "    n_thermal_features = len(df_thermal_columns)\n",
    "    n_tof_features = len(df_tof_columns)\n",
    "\n",
    "    \n",
    "    # Find number of unique sequences\n",
    "    sequence_ids = df_train[\"sequence_id\"].unique()\n",
    "    n_sequences = len(sequence_ids)\n",
    "\n",
    "    # Preallocate final arrays\n",
    "    X_imu_train = np.zeros((n_sequences, max_steps, n_imu_features), dtype=np.float32)\n",
    "    X_thermal_train = np.zeros((n_sequences, max_steps, n_thermal_features), dtype=np.float32)\n",
    "    X_tof_train = np.zeros((n_sequences, max_steps, n_tof_features), dtype=np.float32)\n",
    "\n",
    "    for i, seq_id in enumerate(sequence_ids):\n",
    "        if i % 100 ==0:\n",
    "            print(f\"Processing {i}/{n_sequences}\")\n",
    "\n",
    "        #imu\n",
    "        seq = df_train[df_train[\"sequence_id\"] == seq_id][df_imu_columns].values.astype(np.float32)\n",
    "        seq_len = len(seq)\n",
    "        if seq_len > max_steps:\n",
    "            seq = seq[-max_steps:]\n",
    "        X_imu_train[i, :seq_len, :] = seq\n",
    "        \n",
    "        #thermal\n",
    "        seq = df_train[df_train[\"sequence_id\"] == seq_id][df_thermal_columns].values.astype(np.float32)\n",
    "        seq_len = len(seq)\n",
    "        if seq_len > max_steps:\n",
    "            seq = seq[-max_steps:]\n",
    "        X_thermal_train[i, :seq_len, :] = seq\n",
    "\n",
    "\n",
    "        #tof\n",
    "        seq = df_train[df_train[\"sequence_id\"] == seq_id][df_tof_columns].values.astype(np.float32)\n",
    "        seq_len = len(seq)\n",
    "        if seq_len > max_steps:\n",
    "            seq = seq[-max_steps:]\n",
    "        X_tof_train[i, :seq_len, :] = seq\n",
    "\n",
    "    return X_imu_train,X_thermal_train,X_tof_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/home/ankur/Desktop/ML_DL_Projects/data/cmi-detect-behavior-with-sensor-data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 336)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X with NaNs Before:\n",
      " []\n",
      "Columns in X with NaNs After:\n",
      " []\n",
      "Processing 0/2\n"
     ]
    }
   ],
   "source": [
    "X_imu,X_thermal,X_tof  = prepare_whole_sequences_sumbission(df_test,df_imu_columns,df_thermal_columns,df_tof_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 400, 7), (2, 400, 5), (2, 400, 320))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_imu.shape,X_thermal.shape,X_tof.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.811413, 23.894617, 24.68832 , 24.060965, 24.301336],\n",
       "       [23.742634, 23.97389 , 24.774445, 24.0123  , 24.206102],\n",
       "       [25.122921, 25.442577, 27.227499, 25.478085, 23.924423],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ]],\n",
       "      shape=(400, 5), dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_thermal[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict() -> str:\n",
    "    X_imu,X_thermal,X_tof = prepare_whole_sequences_sumbission(df_test,df_imu_columns,df_thermal_columns,df_tof_columns)\n",
    "    # X_thermal = np.zeros((1,400,5))\n",
    "    print(\"processed test sequence\",X_imu.shape,X_thermal.shape,X_tof.shape)\n",
    "    predition_imu = cmi_imu_18_model.predict(X_imu)\n",
    "    print(\"predition_imu :\",predition_imu)\n",
    "\n",
    "\n",
    "    if (X_thermal== 0).all():\n",
    "      final_predition = predition_imu\n",
    "      print(\"predicint using only imu ###\")\n",
    "    else:\n",
    "      imu_probs = cmi_imu_18_model.predict(X_imu)        # shape (N, 18)\n",
    "      thermal_probs = cmi_thermal_model.predict(X_thermal)  # shape (N, 18)\n",
    "      tof_probs = cmi_tof_model.predict(X_tof)        # shape (N, 18)\n",
    "\n",
    "      stacked_features = np.concatenate([imu_probs, thermal_probs, tof_probs], axis=1)\n",
    "      dval = xgb.DMatrix(stacked_features)\n",
    "      final_predition = booster.predict(dval)\n",
    "      # final_predition = xgb_model.predict_proba(stacked_features)\n",
    "      print(\"predicint using ensable of tree ###\")\n",
    "   \n",
    "    y_pred_int = np.argmax(final_predition, axis=1)\n",
    "    y_pred_labels = encoder.inverse_transform(y_pred_int)\n",
    "    print(\"Predicted labels:\", y_pred_labels[0])\n",
    "    return y_pred_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X with NaNs Before:\n",
      " []\n",
      "Columns in X with NaNs After:\n",
      " []\n",
      "Processing 0/2\n",
      "processed test sequence (2, 400, 7) (2, 400, 5) (2, 400, 320)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "predition_imu : [[4.02438035e-03 6.58133477e-02 2.79653741e-05 2.11638033e-01\n",
      "  1.57056943e-01 3.59789061e-04 2.79140592e-01 2.41146594e-01\n",
      "  2.15150362e-06 1.17265815e-02 1.70467272e-02 5.63825888e-04\n",
      "  9.85531515e-05 4.31082532e-04 1.03425058e-02 1.94016084e-05\n",
      "  3.20472522e-04 2.41052796e-04]\n",
      " [1.36500672e-02 1.12624787e-01 4.20465824e-07 9.79427770e-02\n",
      "  6.29273772e-01 2.98301131e-07 8.98022263e-04 1.30293440e-04\n",
      "  3.53763255e-07 9.68850181e-02 4.83245105e-02 3.70352193e-10\n",
      "  1.37076654e-07 5.47080731e-07 2.68831878e-04 1.97212664e-08\n",
      "  9.33703532e-08 3.98897970e-10]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "predicint using ensable of tree ###\n",
      "Predicted labels: Cheek - pinch skin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cheek - pinch skin'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], shape=(400, 5))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_thermal = np.zeros((1,400,5))\n",
    "X_thermal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_test = sequence.to_pandas()\n",
    "    print(\"received df with shape\",df_test.shape)\n",
    "    \n",
    "    X_imu,X_thermal = prepare_whole_sequences_sumbission(df_test,df_imu_columns,df_thermal_columns)\n",
    "    print(\"processed test sequence\",X_imu.shape,X_thermal.shape)\n",
    "    \n",
    "    predition_imu = cmi_imu_18_model.predict(X_imu)\n",
    "    print(\"predition_imu :\",predition_imu)\n",
    "\n",
    "    def safe_predict(model, X):\n",
    "      if (X == 0).all():\n",
    "        return np.zeros((X.shape[0], model.output_shape[-1]))  # all-zero probabilities\n",
    "      else:\n",
    "        return model.predict(X)\n",
    "          \n",
    "    predition_thermal = safe_predict(cmi_thermal_model_18, X_thermal)\n",
    "    print(\"predition_thermal :\",predition_thermal)\n",
    "\n",
    "    final_predition = predition_imu + predition_thermal\n",
    "    y_pred_int = np.argmax(final_predition, axis=1)\n",
    "    y_pred_labels = encoder.inverse_transform(y_pred_int)\n",
    "    print(\"Predicted labels:\", y_pred_labels[0])\n",
    "    \n",
    "    return y_pred_labels[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
